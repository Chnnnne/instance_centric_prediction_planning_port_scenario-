Model(
  (agent_net): PolylineNet(
    (fc1): Sequential(
      (0): Linear(in_features=13, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU(inplace=True)
    )
    (fc2): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU(inplace=True)
    )
    (fc_out): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=64, out_features=128, bias=True)
    )
  )
  (map_net): PolylineNet(
    (fc1): Sequential(
      (0): Linear(in_features=5, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU(inplace=True)
    )
    (fc2): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU(inplace=True)
    )
    (fc_out): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=64, out_features=128, bias=True)
    )
  )
  (rpe_net): Sequential(
    (0): Linear(in_features=5, out_features=64, bias=True)
    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
  )
  (fusion_net): FusionNet(
    (fusion): ModuleList(
      (0): SftLayer(
        (proj_memory): Sequential(
          (0): Linear(in_features=320, out_features=128, bias=True)
          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (2): ReLU(inplace=True)
        )
        (proj_edge): Sequential(
          (0): Linear(in_features=128, out_features=64, bias=True)
          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (2): ReLU(inplace=True)
        )
        (norm_edge): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (activation): ReLU(inplace=True)
      )
      (1): SftLayer(
        (proj_memory): Sequential(
          (0): Linear(in_features=320, out_features=128, bias=True)
          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (2): ReLU(inplace=True)
        )
        (proj_edge): Sequential(
          (0): Linear(in_features=128, out_features=64, bias=True)
          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (2): ReLU(inplace=True)
        )
        (norm_edge): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (activation): ReLU(inplace=True)
      )
      (2): SftLayer(
        (proj_memory): Sequential(
          (0): Linear(in_features=320, out_features=128, bias=True)
          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (2): ReLU(inplace=True)
        )
        (proj_edge): Sequential(
          (0): Linear(in_features=128, out_features=64, bias=True)
          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (2): ReLU(inplace=True)
        )
        (norm_edge): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (activation): ReLU(inplace=True)
      )
      (3): SftLayer(
        (proj_memory): Sequential(
          (0): Linear(in_features=320, out_features=128, bias=True)
          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (2): ReLU(inplace=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (activation): ReLU(inplace=True)
      )
    )
  )
  (plan_net): PlanNet(
    (plan_mlp): PolylineNet(
      (fc1): Sequential(
        (0): Linear(in_features=4, out_features=128, bias=True)
        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=128, out_features=128, bias=True)
        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
      )
      (fc2): Sequential(
        (0): Linear(in_features=256, out_features=128, bias=True)
        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=128, out_features=128, bias=True)
        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
      )
    )
    (gate): Sequential(
      (0): Linear(in_features=256, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Sigmoid()
    )
    (dropout): Dropout(p=0.3, inplace=False)
  )
  (traj_decoder): TrajDecoder(
    (taregt_prob_layer): Sequential(
      (0): ResMLP(
        (linear1): Linear(in_features=130, out_features=64, bias=True)
        (linear2): Linear(in_features=64, out_features=64, bias=True)
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (act1): ReLU(inplace=True)
        (act2): ReLU(inplace=True)
        (shortcut): Sequential(
          (0): Linear(in_features=130, out_features=64, bias=True)
          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): Linear(in_features=64, out_features=1, bias=True)
    )
    (target_offset_layer): Sequential(
      (0): ResMLP(
        (linear1): Linear(in_features=130, out_features=64, bias=True)
        (linear2): Linear(in_features=64, out_features=64, bias=True)
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (act1): ReLU(inplace=True)
        (act2): ReLU(inplace=True)
        (shortcut): Sequential(
          (0): Linear(in_features=130, out_features=64, bias=True)
          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): Linear(in_features=64, out_features=2, bias=True)
    )
    (motion_estimator_layer): Sequential(
      (0): ResMLP(
        (linear1): Linear(in_features=130, out_features=64, bias=True)
        (linear2): Linear(in_features=64, out_features=64, bias=True)
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (act1): ReLU(inplace=True)
        (act2): ReLU(inplace=True)
        (shortcut): Sequential(
          (0): Linear(in_features=130, out_features=64, bias=True)
          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): Linear(in_features=64, out_features=16, bias=True)
    )
    (traj_prob_layer): Sequential(
      (0): ResMLP(
        (linear1): Linear(in_features=144, out_features=64, bias=True)
        (linear2): Linear(in_features=64, out_features=64, bias=True)
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (act1): ReLU(inplace=True)
        (act2): ReLU(inplace=True)
        (shortcut): Sequential(
          (0): Linear(in_features=144, out_features=64, bias=True)
          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): Linear(in_features=64, out_features=1, bias=True)
      (2): Softmax(dim=2)
    )
  )
)